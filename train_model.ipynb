{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15a00ba",
   "metadata": {},
   "source": [
    "# <h3><b>Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef46fbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import library yang diperlukan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Library berhasil diimport!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0448082",
   "metadata": {},
   "source": [
    "# <h3><b>Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2106e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://pornhub.com</td>\n",
       "      <td>Free Porn Videos &amp; Sex Movies - Porno, XXX, Po...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://xvideos.com</td>\n",
       "      <td>Free Porn Videos - XVIDEOS.COM WARNING This si...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://xhamster.com</td>\n",
       "      <td>Free Porn Videos &amp; Sex Tube Movies at xHamster...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://redtube.com</td>\n",
       "      <td>Free Porn Sex Videos - Redtube - XXX Movies - ...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://xnxx.com</td>\n",
       "      <td>Free Porn, Sex, Tube Videos, XXX Pics, Pussy i...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>https://www.youtube.com/watch?v=0Zvmc-Dbl9Y</td>\n",
       "      <td>Pubertas dan Cara Mengelolanya (Sex Education ...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>https://www.zenius.net/blog/rangkuman-mata-pel...</td>\n",
       "      <td>Rangkuman Mata Pelajaran SD Kelas 1, 2, 3 Leng...</td>\n",
       "      <td>aman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>https://poop.pm/d/8q86e4607b6r</td>\n",
       "      <td>doodshare ukhty boobs mainin mki poophd doodsh...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>https://poop.pm/d/7wlvji64bbeq</td>\n",
       "      <td>anis male friends like like important turn par...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>https://poop.pm/d/8cij6xed6jcr</td>\n",
       "      <td>porn indo angelchan pussy pakai dildo full vid...</td>\n",
       "      <td>berbahaya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0                                  https://pornhub.com   \n",
       "1                                  https://xvideos.com   \n",
       "2                                 https://xhamster.com   \n",
       "3                                  https://redtube.com   \n",
       "4                                     https://xnxx.com   \n",
       "..                                                 ...   \n",
       "380        https://www.youtube.com/watch?v=0Zvmc-Dbl9Y   \n",
       "381  https://www.zenius.net/blog/rangkuman-mata-pel...   \n",
       "382                     https://poop.pm/d/8q86e4607b6r   \n",
       "383                     https://poop.pm/d/7wlvji64bbeq   \n",
       "384                     https://poop.pm/d/8cij6xed6jcr   \n",
       "\n",
       "                                                  text      label  \n",
       "0    Free Porn Videos & Sex Movies - Porno, XXX, Po...  berbahaya  \n",
       "1    Free Porn Videos - XVIDEOS.COM WARNING This si...  berbahaya  \n",
       "2    Free Porn Videos & Sex Tube Movies at xHamster...  berbahaya  \n",
       "3    Free Porn Sex Videos - Redtube - XXX Movies - ...  berbahaya  \n",
       "4    Free Porn, Sex, Tube Videos, XXX Pics, Pussy i...  berbahaya  \n",
       "..                                                 ...        ...  \n",
       "380  Pubertas dan Cara Mengelolanya (Sex Education ...       aman  \n",
       "381  Rangkuman Mata Pelajaran SD Kelas 1, 2, 3 Leng...       aman  \n",
       "382  doodshare ukhty boobs mainin mki poophd doodsh...  berbahaya  \n",
       "383  anis male friends like like important turn par...  berbahaya  \n",
       "384  porn indo angelchan pussy pakai dildo full vid...  berbahaya  \n",
       "\n",
       "[385 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Membaca data dari datatrain.xlsx...\")\n",
    "try:\n",
    "    df = pd.read_excel('datatrain.xlsx')\n",
    "    print(f\"Data berhasil dibaca! Shape: {df.shape}\")\n",
    "    print(\"\\nInfo dataset:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"File datatrain.xlsx tidak ditemukan. Pastikan file berada di direktori yang sama.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error membaca file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6824d0",
   "metadata": {},
   "source": [
    "# <h3><b>Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksplorasi data\n",
    "print(\"\\n=== EKSPLORASI DATA ===\")\n",
    "print(f\"Jumlah data: {len(df)}\")\n",
    "print(f\"Kolom yang tersedia: {df.columns.tolist()}\")\n",
    "print(\"\\nDistribusi label:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nPersentase label:\")\n",
    "print(df['label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Cek missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualisasi distribusi label\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['label'].value_counts().plot(kind='bar', color=['lightcoral', 'lightblue'])\n",
    "plt.title('Distribusi Label Dataset')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Jumlah')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe6ada",
   "metadata": {},
   "source": [
    "# <h3><b>Prepos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk preprocessing teks\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and numbers (opsional)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"\\n=== PREPROCESSING DATA ===\")\n",
    "# Apply preprocessing\n",
    "df['text_processed'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove rows with empty text\n",
    "df = df[df['text_processed'].str.len() > 0]\n",
    "print(f\"Jumlah data setelah preprocessing: {len(df)}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['text_processed']\n",
    "y = df['label']\n",
    "\n",
    "print(f\"Jumlah sampel: {len(X)}\")\n",
    "print(f\"Distribusi label setelah preprocessing:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109b602",
   "metadata": {},
   "source": [
    "# <h3><b> Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== SPLIT DATA ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Data training: {len(X_train)} sampel\")\n",
    "print(f\"Data testing: {len(X_test)} sampel\")\n",
    "print(f\"Distribusi label training: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Distribusi label testing: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Feature extraction menggunakan TF-IDF\n",
    "print(\"\\n=== FEATURE EXTRACTION ===\")\n",
    "# Inisialisasi TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Maksimal 5000 fitur\n",
    "    min_df=2,          # Minimal muncul di 2 dokumen\n",
    "    max_df=0.8,        # Maksimal muncul di 80% dokumen\n",
    "    stop_words='english',  # Hapus stop words bahasa Inggris\n",
    "    ngram_range=(1, 2)     # Unigram dan bigram\n",
    ")\n",
    "\n",
    "# Fit dan transform data training\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"Shape X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape X_test_tfidf: {X_test_tfidf.shape}\")\n",
    "print(f\"Jumlah fitur TF-IDF: {len(tfidf.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895def71",
   "metadata": {},
   "source": [
    "# <h3><b>Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVM Model\n",
    "print(\"\\n=== TRAINING SVM MODEL ===\")\n",
    "# Inisialisasi SVM classifier\n",
    "svm_model = SVC(\n",
    "    kernel='linear',    # Kernel linear untuk text classification\n",
    "    C=1.0,             # Regularization parameter\n",
    "    random_state=42,\n",
    "    probability=True   # Untuk mendapatkan probabilitas prediksi\n",
    ")\n",
    "\n",
    "# Training model\n",
    "print(\"Memulai training...\")\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "print(\"Training selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc55be5",
   "metadata": {},
   "source": [
    "# <h3><b>Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caabf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi pada data test\n",
    "print(\"\\n=== EVALUASI MODEL ===\")\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "y_pred_proba = svm_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Evaluasi performa\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Akurasi: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=svm_model.classes_, \n",
    "            yticklabels=svm_model.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisis fitur penting\n",
    "print(\"\\n=== ANALISIS FITUR PENTING ===\")\n",
    "# Mendapatkan koefisien fitur\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coefficients = svm_model.coef_[0]\n",
    "\n",
    "# Membuat DataFrame untuk analisis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 fitur paling penting (positif - berbahaya):\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "print(\"\\nTop 10 fitur paling penting (negatif - aman):\")\n",
    "print(feature_importance.tail(10))\n",
    "\n",
    "# Visualisasi top fitur\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = pd.concat([feature_importance.head(10), feature_importance.tail(10)])\n",
    "colors = ['red' if x > 0 else 'blue' for x in top_features['coefficient']]\n",
    "plt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Koefisien SVM')\n",
    "plt.title('Top 20 Fitur Paling Penting')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d1ab",
   "metadata": {},
   "source": [
    "# <h3><b>Prediksi Data Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk prediksi teks baru\n",
    "def predict_text(text, model=svm_model, vectorizer=tfidf):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memprediksi teks baru\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Vectorization\n",
    "    text_tfidf = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Prediksi\n",
    "    prediction = model.predict(text_tfidf)[0]\n",
    "    probability = model.predict_proba(text_tfidf)[0]\n",
    "    \n",
    "    # Confidence score\n",
    "    confidence = max(probability)\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': dict(zip(model.classes_, probability))\n",
    "    }\n",
    "\n",
    "# Test prediksi dengan contoh\n",
    "print(\"\\n=== TEST PREDIKSI ===\")\n",
    "test_texts = [\n",
    "    \"Click here to win free money now!\",\n",
    "    \"This is a normal website about technology\",\n",
    "    \"Download malware and virus for free\",\n",
    "    \"Learn Python programming tutorial\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_text(text)\n",
    "    print(f\"\\nTeks: '{text}'\")\n",
    "    print(f\"Prediksi: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"Probabilitas: {result['probabilities']}\")\n",
    "\n",
    "# Simpan model (opsional)\n",
    "print(\"\\n=== SIMPAN MODEL ===\")\n",
    "import joblib\n",
    "\n",
    "# Simpan model dan vectorizer\n",
    "joblib.dump(svm_model, 'svm_model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "print(\"Model dan vectorizer berhasil disimpan!\")\n",
    "\n",
    "# Instruksi untuk load model\n",
    "print(\"\\n=== CARA LOAD MODEL ===\")\n",
    "print(\"Untuk load model yang sudah disimpan:\")\n",
    "print(\"loaded_model = joblib.load('svm_model.pkl')\")\n",
    "print(\"loaded_tfidf = joblib.load('tfidf_vectorizer.pkl')\")\n",
    "\n",
    "print(\"\\n=== TRAINING SELESAI ===\")\n",
    "print(f\"Model SVM berhasil ditraining dengan akurasi: {accuracy:.4f}\")\n",
    "print(\"Model siap digunakan untuk klasifikasi teks!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snailly-wowo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
